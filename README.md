# DATA-PIPELINE-DEVELOPMENT
COMPANY: CODTECH IT SOLUTIONS

NAME: SIVAKALA KIRAN KUMAR

INTERN ID: CT12QYV

DURATION: 8 WEEKS

MENTOR: NEELA SANTHU

Description:
In data science, raw data is rarely ready for immediate analysis. It often comes from multiple sources, in different formats, and may contain missing or inconsistent values. To make this data useful, a structured and automated process is required â€” this is where data pipelines come into play.

A data pipeline is a set of tools and processes used to move data from source systems to a storage or analysis platform, such as a data warehouse or machine learning model. It typically follows the ETL process: Extract, Transform, and Load.

Extract: Data is collected from various sources such as APIs, databases, or sensor logs.

Transform: The data is cleaned, formatted, and sometimes enriched. This step involves removing duplicates, handling missing values, and converting data types.

Load: The cleaned data is stored in a target system where it can be accessed for analysis or modeling.

Pipelines can operate in batch mode (periodic updates) or in real time (streaming data). Real-time pipelines are especially useful in use cases like fraud detection, recommendation systems, or monitoring systems where timely data is critical.
